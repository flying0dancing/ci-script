hadoop.user=root
env.hostname=platform.tools.docker

env.hdfs.url=hdfs://${env.hostname}:9000

spark.libs.copy=true
logging.config=ignis-server-parent/ignis-server/src/test/resources/logback-test.xml
spring.datasource.url=jdbc:h2:./${ignis.home}/data/ignis;MODE=Oracle;AUTO_SERVER=TRUE

#This is needed to avoid bug with hdfs client talking to data node's internal ip instead of hostname
hadoop.site.conf.dfs.client.use.datanode.hostname=true
spark-defaults.conf.dfs.client.use.datanode.hostname=true

spark-defaults.conf.spark.driver.extraClassPath=/root/platform-tools/hadoop-2.8.3/etc/hadoop:/root/platform-tools/hadoop-2.8.3/share/hadoop/common/lib/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/common/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/hdfs:/root/platform-tools/hadoop-2.8.3/share/hadoop/hdfs/lib/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/hdfs/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/yarn/lib/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/yarn/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/mapreduce/lib/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/mapreduce/*:/root/platform-tools/hadoop-2.8.3/contrib/capacity-scheduler/*.jar
spark-defaults.conf.spark.executor.extraClassPath=/root/platform-tools/hadoop-2.8.3/etc/hadoop:/root/platform-tools/hadoop-2.8.3/share/hadoop/common/lib/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/common/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/hdfs:/root/platform-tools/hadoop-2.8.3/share/hadoop/hdfs/lib/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/hdfs/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/yarn/lib/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/yarn/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/mapreduce/lib/*:/root/platform-tools/hadoop-2.8.3/share/hadoop/mapreduce/*:/root/platform-tools/hadoop-2.8.3/contrib/capacity-scheduler/*.jar

spark-defaults.conf.spark.driver.memory=512M
spark-defaults.conf.spark.executor.memory=${spark-defaults.conf.spark.driver.memory}
spark-defaults.conf.spark.executor.cores=1
spark-defaults.conf.spark.executor.instances=${spark-defaults.conf.spark.executor.cores}
spark-defaults.conf.spark.default.parallelism=1

#QA Cluster version
#spark-defaults.conf.spark.driver.extraClassPath=/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/etc/hadoop:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/common/lib/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/common/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/hdfs:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/hdfs/lib/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/hdfs/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/yarn/lib/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/yarn/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/mapreduce/lib/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/mapreduce/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/contrib/capacity-scheduler/*.jar
#spark-defaults.conf.spark.executor.extraClassPath=/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/etc/hadoop:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/common/lib/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/common/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/hdfs:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/hdfs/lib/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/hdfs/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/yarn/lib/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/yarn/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/mapreduce/lib/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/share/hadoop/mapreduce/*:/home/ubuntu/fcr-engine/platform-tools/2.3.0-b1020/hadoop-2.8.3/contrib/capacity-scheduler/*.jar
