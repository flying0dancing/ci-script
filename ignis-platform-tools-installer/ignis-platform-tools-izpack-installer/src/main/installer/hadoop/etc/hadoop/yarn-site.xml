<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>%{yarn.rm.host}</value>
    <description>The hostname of the Resource Manager</description>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>256</value>
    <description>Minimum limit of memory to allocate to each container request at the Resource Manager.</description>
  </property>

  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>4096</value>
    <description>Maximum limit of memory to allocate to each container request at the Resource Manager.</description>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
    <description>The minimum allocation for every container request at the RM, in terms of virtual CPU cores. Requests
      lower than this won't take effect, and the specified value will get allocated the minimum.
    </description>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>4</value>
    <description>The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests
      higher than this won't take effect, and will get capped to this value.
    </description>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>%{yarn.nodemanager.resource.memory-mb}</value>
    <description>Physical memory, in MB, to be made available to running containers</description>
  </property>
  <property>
    <name>yarn.nodemanager.log.retain-second</name>
    <value>604800</value>
  </property>

  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>%{yarn.nodemanager.resource.cpu-vcores}</value>
    <description>Number of CPU cores that can be allocated for containers.</description>
  </property>

  <!-- logging -->
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.log.server.url</name>
    <value>%{yarn.log.server.url}</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>2592000</value>
  </property>

  <!--  http://stackoverflow.com/questions/29512565/spark-pi-example-in-cluster-mode-with-yarn-association-lost -->
  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>%{yarn.nodemanager.pmem-check-enabled}</value>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>%{yarn.nodemanager.vmem-check-enabled}</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
  </property>
</configuration>