logging.file=${logging.file}
logging.config=${logging.config}
logging.pattern.console=%d{YYYY-MM-dd HH:mm:ss.SSS} %clr(%5p) %clr(%X{correlationId}){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n
logging.pattern.file=%d{YYYY-MM-dd HH:mm:ss.SSS} %clr(%5p) %clr(%X{correlationId}){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n

spring.application.name=ignis
spring.datasource.url=${database.jdbc.url}
spring.datasource.username=${database.connection.user}
spring.datasource.password=${database.connection.password}
spring.datasource.driver-class-name=${database.jdbc.driver}
spring.datasource.db.schema=${spring.datasource.db.schema}
spring.datasource.tomcat.validationQuery=${spring.datasource.validation.query}

togglz.console.enabled=${togglz.console.enabled}
togglz.features-file=${togglz.features-file}
togglz.console.use-management-port=false
togglz.console.path=/togglz
togglz.console.secured=false
togglz.endpoint.enabled=false
togglz.endpoint.id=togglz

phoenix.datasource.url=${phoenix.datasource.url}
phoenix.datasource.driver-class-name=${phoenix.datasource.driver-class-name}
phoenix.salt.bucket.count=${phoenix.salt.bucket.count}

spring.jpa.show-sql=false
spring.jpa.hibernate.ddl-auto=${spring.jpa.hibernate.ddl-auto}
spring.jpa.hibernate.naming.physical-strategy=org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
spring.jpa.hibernate.use-new-id-generator-mappings=${database.hibernate.new.id.generator}
spring.jpa.properties.hibernate.dialect=${database.hibernate.dialect}
spring.jpa.properties.hibernate.jdbc.batch_size=20
spring.jpa.properties.hibernate.order_inserts=true
spring.jpa.properties.hibernate.order_updates=true

spring.batch.job.enabled=false
spring.batch.initializer.enabled=false

management.endpoints.web.exposure.include=health,info,metrics
management.endpoint.metrics.enabled=true
management.metrics.web.server.auto-time-requests=true
management.endpoint.health.show-details=when_authorized
management.health.hadoop.enabled=${management.health.hadoop.enabled}

management.metrics.export.datadog.api-key=${management.metrics.export.datadog.api-key}
management.metrics.export.datadog.enabled=${management.metrics.export.datadog.enabled}
management.metrics.export.datadog.host-tag=${management.metrics.export.datadog.host-tag}
management.metrics.export.datadog.step=${management.metrics.export.datadog.step}

dataset.source.location.type=${dataset.source.location.type}
dataset.source.location.localPath=${dataset.source.location.localPath}
dataset.source.location.remotePath=${dataset.source.location.remotePath}
dataset.source.location.s3.credentialsSource=${dataset.source.location.s3.credentialsSource}
dataset.source.location.s3.bucket=${dataset.source.location.s3.bucket}
dataset.source.location.s3.region=${dataset.source.location.s3.region}
dataset.source.location.s3.prefix=${dataset.source.location.s3.prefix}
dataset.error.location.s3.bucket=${dataset.error.location.s3.bucket}
dataset.error.location.s3.prefix=${dataset.error.location.s3.prefix}

s3.protocol=${s3.protocol}

spring.jackson.default-property-inclusion=non_null
spring.jackson.serialization.indent_output=true

hadoop.user=${hadoop.user}
hadoop.site.conf.fs.defaultFS=${fs.defaultFS}
hadoop.site.conf.yarn.resourcemanager.hostname=${yarn.resourcemanager.hostname}
hadoop.site.conf.yarn.nodemanager.pmem-check-enabled=${yarn.nodemanager.pmem-check-enabled}
hadoop.site.conf.yarn.nodemanager.vmem-check-enabled=${yarn.nodemanager.vmem-check-enabled}

spark.drivers.pipeline.resource=${spark.drivers.pipeline.resource}
spark.drivers.pipeline.mainClass=com.lombardrisk.ignis.spark.Application
spark.drivers.validation.resource=${spark.drivers.validation.resource}
spark.drivers.validation.mainClass=com.lombardrisk.ignis.spark.Application
spark.drivers.staging.resource=${spark.drivers.staging.resource}
spark.drivers.staging.mainClass=com.lombardrisk.ignis.spark.Application

spark.extra.jars.dir=${spark.extra.jars.dir}

spark-defaults.conf.debug.mode=${spark-defaults.conf.debug.mode}
spark-defaults.conf.spark.eventLog.enabled=${spark.eventLog.enabled}
spark-defaults.conf.spark.eventLog.compress=${spark.eventLog.compress}
spark-defaults.conf.spark.eventLog.dir=${spark.eventLog.dir}
spark-defaults.conf.spark.history.fs.logDirectory=${spark.eventLog.dir}
spark-defaults.conf.spark.history.ui.port=${spark.history.server.port}
spark-defaults.conf.spark.submit.deployMode=cluster
spark-defaults.conf.spark.yarn.submit.waitAppCompletion=true
spark-defaults.conf.spark.yarn.historyServer.address=${spark.yarn.historyServer.address}
spark-defaults.conf.spark.yarn.maxAppAttempts=${spark.yarn.maxAppAttempts}
spark-defaults.conf.spark.yarn.archive=${spark.yarn.archive}
spark-defaults.conf.spark.driver.memory=${spark.driver.memory}
spark-defaults.conf.spark.executor.memory=${spark.executor.memory}
spark-defaults.conf.spark.executor.cores=${spark.executor.cores}
spark-defaults.conf.spark.executor.instances=${spark.executor.instances}
spark-defaults.conf.spark.default.parallelism=${spark.default.parallelism}
spark-defaults.conf.spark.hadoop.fs.defaultFS=${fs.defaultFS}
spark-defaults.conf.spark.hadoop.yarn.resourcemanager.hostname=${yarn.resourcemanager.hostname}
spark-defaults.conf.spark.hadoop.yarn.nodemanager.pmem-check-enabled=${yarn.nodemanager.pmem-check-enabled}
spark-defaults.conf.spark.hadoop.yarn.nodemanager.vmem-check-enabled=${yarn.nodemanager.vmem-check-enabled}
spark-defaults.conf.spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j.properties
spark-defaults.conf.spark.driver.extraClassPath=${spark.driver.extraClassPath}
spark-defaults.conf.spark.driver.extraLibraryPath=${spark.driver.extraLibraryPath}
spark-defaults.conf.spark.executor.extraClassPath=${spark.executor.extraClassPath}
spark-defaults.conf.spark.executor.extraLibraryPath=${spark.executor.extraLibraryPath}
spark-defaults.conf.hadoop.user=${hadoop.user}
spark-defaults.conf.zookeeper.url=${zookeeper.hosts}:${zookeeper.client.port}
spark-defaults.conf.keystore.password=${server.ssl.key-store-password}
spark-defaults.conf.ignis.host=${ignis.host}
spark-defaults.conf.server.https.port=${server.https.port}
spark-defaults.conf.phoenix.salt.bucket.count=${phoenix.salt.bucket.count}
spark-defaults.conf.job.user.name=InternalJobUser
spark-defaults.conf.job.user.password=!qod;a3p8if#_=@KV
spark-defaults.conf.server.servlet.contextPath=${server.servlet.contextPath}

spark.log4j.file=${spark.log4j.file}
spark.yarn.app.tracking.resource.url=${spark.yarn.app.tracking.resource.url}

spark.libs.path=${spark.libs.path}
spark.libs.copy=${spark.libs.copy}

ignis.host=${ignis.host}
ignis.environment=${ignis.environment}
ignis.home=${IGNIS_HOME}
ignis.ui.home=${ignis.ui.home}
ignis.ui.enabled=true
ignis.tmp.path=${tmp.path}

server.https.port=${server.https.port}
server.http.port=${server.http.port}
server.port=${server.https.port}

server.ssl.key-store-type=JKS
server.ssl.key-store=${server.ssl.key-store}
server.ssl.key-password=${server.ssl.key-password}
server.ssl.key-store-password=${server.ssl.key-store-password}
server.servlet.contextPath=/${server.servlet.contextPath}/
server.ssl.enabled-protocols=${server.ssl.enabled-protocols}
server.ssl.protocol=${server.ssl.protocol}
server.ssl.ciphers=${server.ssl.ciphers}

server.tomcat.accesslog.enabled=true
server.tomcat.basedir=${server.tomcat.basedir}
server.tomcat.accesslog.pattern=[%{YYYY-MM-dd HH:mm:ss Z}t] %h X-Forwarded-For:%{X-Forwarded-For}i \"%r\" %s %b

spring.servlet.multipart.max-file-size=5MB
spring.servlet.multipart.max-request-size=5MB

export.dataset.page-size=50
export.dataset.max-records=7000
